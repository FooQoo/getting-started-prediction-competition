{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set file path\n",
    "filepath = '../fact/'\n",
    "trainfile = 'train.csv'\n",
    "testfile = 'test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read train.csv\n",
    "import pandas as pd\n",
    "df_train = pd.read_csv(filepath+trainfile)\n",
    "df_test = pd.read_csv(filepath+testfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1:0.8554968795007201\n",
      "[[2673  271]\n",
      " [ 632 2312]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.91      0.86      2944\n",
      "           1       0.90      0.79      0.84      2944\n",
      "\n",
      "    accuracy                           0.85      5888\n",
      "   macro avg       0.85      0.85      0.85      5888\n",
      "weighted avg       0.85      0.85      0.85      5888\n",
      "\n",
      "f1:0.823793490460157\n",
      "[[367  68]\n",
      " [ 89 238]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.84      0.82       435\n",
      "           1       0.78      0.73      0.75       327\n",
      "\n",
      "    accuracy                           0.79       762\n",
      "   macro avg       0.79      0.79      0.79       762\n",
      "weighted avg       0.79      0.79      0.79       762\n",
      "\n",
      "f1:0.8177934154310129\n",
      "[[1627  234]\n",
      " [ 491  911]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.87      0.82      1861\n",
      "           1       0.80      0.65      0.72      1402\n",
      "\n",
      "    accuracy                           0.78      3263\n",
      "   macro avg       0.78      0.76      0.77      3263\n",
      "weighted avg       0.78      0.78      0.77      3263\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#from sklearn.naive_bayes import MultinomialNB\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score\n",
    "\n",
    "def get_text_model(train, valid):    \n",
    "    prefix = 'word_'\n",
    "    c_word = [column for column in train.columns.tolist() if prefix == column[:len(prefix)]]\n",
    "    prefix = 'url_'\n",
    "    c_url = [column for column in train.columns.tolist() if prefix == column[:len(prefix)]]\n",
    "    prefix = 'hashtag_'\n",
    "    c_hashtag = [column for column in train.columns.tolist() if prefix == column[:len(prefix)]]\n",
    "    \n",
    "    # fill nan\n",
    "    train.fillna(0, inplace=True)\n",
    "    \n",
    "    X_train, X_valid, X_test = train[c_word+c_url+c_hashtag].values, valid[c_word+c_url+c_hashtag].values, test[c_word+c_url+c_hashtag].values\n",
    "    y_train, y_valid = train.target.values, valid.target.values\n",
    "    \n",
    "    # fit model\n",
    "    lgb_train = lgb.Dataset(X_train, y_train)\n",
    "    lgb_valid = lgb.Dataset(X_valid, y_valid, reference=lgb_train)\n",
    "    lgb_test = lgb.Dataset(X_test)\n",
    "    \n",
    "    lgbm_params = {\n",
    "        'objective': 'binary',\n",
    "        'metric':'binary_logloss', \n",
    "        'verbose': -1,\n",
    "        'learning_rate': 0.01,\n",
    "        'num_iterations': 1000\n",
    "    }\n",
    "\n",
    "    model = lgb.train(\n",
    "        lgbm_params, \n",
    "        lgb_train, \n",
    "        valid_sets=lgb_valid,\n",
    "        verbose_eval=False,\n",
    "        early_stopping_rounds=10\n",
    "    )\n",
    "    \n",
    "    def report(X, y):\n",
    "        # print train report\n",
    "        y_pred = model.predict(X, num_iteration=model.best_iteration)\n",
    "        y_pred_cls = y_pred >= 0.5\n",
    "        print('f1:{}'.format(f1_score(y, y_pred_cls, average=None)[0]))\n",
    "        print(confusion_matrix(y, y_pred_cls))\n",
    "        print(classification_report(y, y_pred_cls))\n",
    "    \n",
    "    report(X_train, y_train)\n",
    "    report(X_valid, y_valid)\n",
    "    \n",
    "    # fit train and valid\n",
    "    X = np.concatenate([X_train, X_valid], 0)\n",
    "    y = np.concatenate([y_train, y_valid], 0)\n",
    "    lgb_train_valid = lgb.Dataset(X, y)\n",
    "    model = lgb.train(\n",
    "        lgbm_params, \n",
    "        lgb_train_valid, \n",
    "        verbose_eval=False\n",
    "    )\n",
    "    \n",
    "    report(X_test, y_test)\n",
    "    \n",
    "    # retrun proba\n",
    "    return (\n",
    "        model.predict(X_train, num_iteration=model.best_iteration), \n",
    "        model.predict(X_valid, num_iteration=model.best_iteration),\n",
    "        model.predict(X_test, num_iteration=model.best_iteration)\n",
    "    )\n",
    "\n",
    "y_train_text_proba, y_test_text_proba = get_text_model(df_train, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1:0.7745313291673704\n",
      "[[2293  651]\n",
      " [ 684 2260]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.78      0.77      2944\n",
      "           1       0.78      0.77      0.77      2944\n",
      "\n",
      "    accuracy                           0.77      5888\n",
      "   macro avg       0.77      0.77      0.77      5888\n",
      "weighted avg       0.77      0.77      0.77      5888\n",
      "\n",
      "f1:0.7765830346475507\n",
      "[[325 110]\n",
      " [ 77 250]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.75      0.78       435\n",
      "           1       0.69      0.76      0.73       327\n",
      "\n",
      "    accuracy                           0.75       762\n",
      "   macro avg       0.75      0.76      0.75       762\n",
      "weighted avg       0.76      0.75      0.76       762\n",
      "\n",
      "f1:0.7518267929634641\n",
      "[[1389  472]\n",
      " [ 445  957]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.75      0.75      1861\n",
      "           1       0.67      0.68      0.68      1402\n",
      "\n",
      "    accuracy                           0.72      3263\n",
      "   macro avg       0.71      0.71      0.71      3263\n",
      "weighted avg       0.72      0.72      0.72      3263\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from catboost import CatBoost\n",
    "from catboost import Pool\n",
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score\n",
    "\n",
    "def get_category_model(train, valid, test):\n",
    "    c_text = ['keyword', 'location']\n",
    "    X_train, X_valid, X_test = train[c_text].values, valid[c_text].values, test[c_text].values\n",
    "    y_train, y_valid = train.target, valid.target\n",
    "    \n",
    "    # CatBoost が扱うデータセットの形式に直す\n",
    "    train_pool = Pool(X_train, label=y_train)\n",
    "    valid_pool = Pool(X_valid, label=y_valid)\n",
    "    test_pool = Pool(X_test)\n",
    "\n",
    "    # 学習用のパラメータ\n",
    "    params = {\n",
    "        # タスク設定と損失関数\n",
    "        'loss_function': 'Logloss',\n",
    "        # 学習ラウンド数\n",
    "        'num_boost_round': 1000,\n",
    "        'eval_metric': 'F1',\n",
    "        'silent': False,\n",
    "        'verbose': None,\n",
    "        'early_stopping_rounds': 10\n",
    "    }\n",
    "\n",
    "    # モデルを学習する\n",
    "    model = CatBoost(params)\n",
    "    model.fit(train_pool, logging_level='Silent')\n",
    "    \n",
    "    def report(X_pool, y):\n",
    "        y_pred = model.predict(X_pool, prediction_type='Class')\n",
    "        print('f1:{}'.format(f1_score(y, y_pred, average=None)[0]))\n",
    "        print(confusion_matrix(y, y_pred))\n",
    "        print(classification_report(y, y_pred))\n",
    "    \n",
    "    report(train_pool, y_train)\n",
    "    report(valid_pool, y_valid)\n",
    "    \n",
    "    # fit train and valid\n",
    "    X = np.concatenate([X_train, X_valid], 0)\n",
    "    y = np.concatenate([y_train, y_valid], 0)\n",
    "    pool = Pool(X, label=y)\n",
    "    model.fit(pool, logging_level='Silent')\n",
    "    \n",
    "    report(test_pool, y_test)\n",
    "    \n",
    "    # retrun proba\n",
    "    return (\n",
    "        model.predict(train_pool, prediction_type='Probability')[:, 1], \n",
    "        model.predict(valid_pool, prediction_type='Probability')[:, 1],\n",
    "        model.predict(test_pool, prediction_type='Probability')[:, 1]\n",
    "    )\n",
    "\n",
    "y_train_cat_proba, y_valid_cat_proba, y_test_cat_proba = get_category_model(df_train, df_valid, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1:0.8547120418848166\n",
      "[[2612  332]\n",
      " [ 556 2388]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.89      0.85      2944\n",
      "           1       0.88      0.81      0.84      2944\n",
      "\n",
      "    accuracy                           0.85      5888\n",
      "   macro avg       0.85      0.85      0.85      5888\n",
      "weighted avg       0.85      0.85      0.85      5888\n",
      "\n",
      "f1:0.8849162011173185\n",
      "[[396  39]\n",
      " [ 64 263]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.91      0.88       435\n",
      "           1       0.87      0.80      0.84       327\n",
      "\n",
      "    accuracy                           0.86       762\n",
      "   macro avg       0.87      0.86      0.86       762\n",
      "weighted avg       0.87      0.86      0.86       762\n",
      "\n",
      "f1:0.8153489569920165\n",
      "[[1583  278]\n",
      " [ 439  963]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.85      0.82      1861\n",
      "           1       0.78      0.69      0.73      1402\n",
      "\n",
      "    accuracy                           0.78      3263\n",
      "   macro avg       0.78      0.77      0.77      3263\n",
      "weighted avg       0.78      0.78      0.78      3263\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "\n",
    "X_train = np.stack([y_train_text_proba, y_train_cat_proba], 1)\n",
    "X_valid = np.stack([y_valid_text_proba, y_valid_cat_proba], 1)\n",
    "X_test = np.stack([y_test_text_proba, y_test_cat_proba], 1)\n",
    "y_train, y_valid = df_train.target, df_valid.target\n",
    "\n",
    "clf = LogisticRegression(\n",
    "    class_weight = 'balanced',\n",
    "    random_state = 0,\n",
    "    penalty = 'elasticnet',\n",
    "    l1_ratio = 0.0, \n",
    "    C = 0.001,\n",
    "    solver='saga'\n",
    ")\n",
    "\n",
    "def report(X, y):\n",
    "    y_pred = clf.predict(X)\n",
    "    print('f1:{}'.format(f1_score(y, y_pred, average=None)[0]))\n",
    "    print(confusion_matrix(y, y_pred))\n",
    "    print(classification_report(y, y_pred))\n",
    "\n",
    "# 再学習\n",
    "X = np.concatenate([X_train, X_valid], 0)\n",
    "y = np.concatenate([y_train, y_valid], 0)\n",
    "clf.fit(X, y)\n",
    "\n",
    "report(X_train, y_train)\n",
    "report(X_valid, y_valid)\n",
    "report(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submit.to_csv('../output/submit.csv', index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
